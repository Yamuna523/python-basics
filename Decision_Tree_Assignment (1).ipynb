{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05MEiWg80W5G"
      },
      "source": [
        "# Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hcld7nFG0r4q"
      },
      "source": [
        "```\n",
        "Question 1: What is a Decision Tree, and how does it work in classification?\n",
        "A.A Decision Tree is a supervised machine learning algorithm used for classification and regression.\n",
        "It works like a flowchart where each internal node represents a decision based on a feature\n",
        " ->Each branch represents an outcome of the decision\n",
        " ->Each leaf node represents the final prediction (class label)\n",
        "\n",
        " classification:\n",
        " 1.The algorithm selects the best feature to split the dataset.\n",
        " 2.The split divides the dataset into subsets that are more “pure” (more similar class labels).\n",
        " 3.This splitting continues recursively until all samples belong to the same class or No further splits improve purity, or  stopping condition is reached (like max_depth)\n",
        "\n",
        "The final class is predicted based on the class majority in the leaf node.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaQZ9qCT1Ywm"
      },
      "source": [
        "```\n",
        "Question 2: Explain Gini Impurity and Entropy. How do they impact Decision Tree splits?\n",
        "\n",
        "Gini Impurity:\n",
        "Measures the probability of incorrectly classifying a randomly chosen element.\n",
        "\n",
        "$$ Gini = 1 - \\sum_{i=1}^{k} p_i^2 $$\n",
        "\n",
        "* Value = 0 → perfectly pure node\n",
        "\n",
        "* Used by default in CART decision trees\n",
        "\n",
        "Entropy:\n",
        "\n",
        "Measures the amount of uncertainty or disorder\n",
        "\n",
        "$$ Entropy = - \\sum_{i=1}^{k} p_i \\log_2(p_i) $$\n",
        "\n",
        "* Value = 0 → pure node\n",
        "\n",
        "* Used in ID3 / C4.5 trees\n",
        "\n",
        "**Impact on Splits**\n",
        "\n",
        "The tree chooses the feature and threshold that produces the greatest reduction in impurity.\n",
        "\n",
        "More reduction in impurity → better split → more homogeneous child nodes.\n",
        "\n",
        "Both impurity measures help the tree find the most informative features.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2TooaKh2gBI"
      },
      "source": [
        "```\n",
        "Question 3: Difference between Pre-Pruning and Post-Pruning Pre-Pruning (Early Stopping)\n",
        "\n",
        "Pre-pruning:- stops the decision tree during the training phase before it becomes too complex.\n",
        "It applies certain rules so the tree does not grow unnecessarily.\n",
        "\n",
        "Key points:\n",
        "\n",
        "1)Tree stops splitting early based on constraints.\n",
        "2)Prevents overfitting by limiting depth, nodes, samples, etc.\n",
        "3)Faster to train.\n",
        "4)Risk: Might underfit if stopped too soon.\n",
        "\n",
        "Examples of Pre-Pruning parameters: max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes,min_impurity_decrease\n",
        "\n",
        "Post-pruning:- Allows the tree to grow fully and then removes branches that do not improve model performance.\n",
        "\n",
        "Key points:\n",
        "1)First grow full tree → then cut unnecessary branches.\n",
        "2)More accurate than pre-pruning.\n",
        "3)Helps reduce overfitting after tree is built.\n",
        "4)More computationally expensive.\n",
        "\n",
        "Post-Pruning methods: Cost Complexity Pruning (Alpha pruning), Reduced Error Pruning, Minimum Error Pruning\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1oeKbKmAG4U"
      },
      "source": [
        "Question 4: What is Information Gain and why is it important?\n",
        "A.Information Gain measures the reduction in impurity after a dataset split.\n",
        "\n",
        "$$ IG = H(parent) - \\sum_{j=1}^{m} \\left( \\frac{n_j}{n} \\right) H(child_j) $$\n",
        "\n",
        "Importance:-\n",
        "1)Helps select the best feature for splitting.\n",
        "2)Higher Information Gain → more useful split.\n",
        "3)Ensures the decision tree learns the most informative patterns.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak91iC1xA-UP"
      },
      "source": [
        "**Question 5: Real-world applications of Decision Trees + Advantages & Limitations Applications**\n",
        "\n",
        "**Answer**\n",
        "\n",
        "* Medical diagnosis (disease prediction)\n",
        "\n",
        "* Fraud detection\n",
        "\n",
        "* Customer churn prediction\n",
        "\n",
        "* Loan approval\n",
        "\n",
        "* Recommendation systems\n",
        "\n",
        "* Manufacturing quality checks\n",
        "\n",
        "**Advantages**\n",
        "\n",
        "* Easy to understand and visualize\n",
        "* Handles numerical + categorical data\n",
        "* Requires little data preprocessing\n",
        "* Fast and efficient\n",
        "\n",
        "**Limitations**\n",
        "\n",
        "* Prone to overfitting\n",
        "* Unstable (small changes → different trees)\n",
        "* Can create biased splits if class imbalance exists"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNnoa106CBJj"
      },
      "source": [
        "**Dataset Info:**\n",
        "\n",
        "* Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).\n",
        "\n",
        "* Boston Housing Dataset for regression tasks (sklearn.datasets.load_boston() or provided CSV).\n",
        "\n",
        "**Question 6: Write a Python program to:**\n",
        "* Load the Iris Dataset\n",
        "* Train a Decision Tree Classifier using the Gini criterion\n",
        "* Print the model’s accuracy and feature importances\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHllc7j9AuvF",
        "outputId": "917f2176-f3fb-41a5-d22d-605b5ce8bf0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "sepal length (cm) : 0.0\n",
            "sepal width (cm) : 0.016670139612419255\n",
            "petal length (cm) : 0.9061433868879218\n",
            "petal width (cm) : 0.07718647349965893\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model using Gini\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Feature Importances\n",
        "for name, importance in zip(iris.feature_names, model.feature_importances_):\n",
        "    print(name, \":\", importance)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnndnVMACj3t"
      },
      "source": [
        "**Question 7: Write a Python program to:**\n",
        "\n",
        "* Load the Iris Dataset\n",
        "* Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mgZ-mMIsCduu",
        "outputId": "169de2bc-618b-43d3-9a30-2aa7f8e6bdb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Tree Accuracy: 1.0\n",
            "Max Depth=3 Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Full tree\n",
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "full_pred = full_tree.predict(X_test)\n",
        "\n",
        "# Depth-limited tree\n",
        "limited_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "limited_tree.fit(X_train, y_train)\n",
        "limited_pred = limited_tree.predict(X_test)\n",
        "\n",
        "print(\"Full Tree Accuracy:\", accuracy_score(y_test, full_pred))\n",
        "print(\"Max Depth=3 Accuracy:\", accuracy_score(y_test, limited_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ-sRdmoC5ku"
      },
      "source": [
        "**Question 8: Write a Python program to:**\n",
        "\n",
        "* Load the Boston Housing Dataset\n",
        "* Train a Decision Tree Regressor\n",
        "* Print the Mean Squared Error (MSE) and feature importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KA-azNVtDMrB",
        "outputId": "16333e7d-2da4-4e8c-c9f3-55d5e9589f16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.495235205629094\n",
            "\n",
            "Feature Importances:\n",
            "MedInc : 0.5285090936963706\n",
            "HouseAge : 0.05188353710616045\n",
            "AveRooms : 0.05297496833123543\n",
            "AveBedrms : 0.02866045788296106\n",
            "Population : 0.030515676373806224\n",
            "AveOccup : 0.13083767753210346\n",
            "Latitude : 0.09371656401749287\n",
            "Longitude : 0.08290202505986989\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing Dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Decision Tree Regressor\n",
        "model = DecisionTreeRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# MSE\n",
        "print(\"Mean Squared Error (MSE):\", mean_squared_error(y_test, y_pred))\n",
        "\n",
        "# Feature importance\n",
        "print(\"\\nFeature Importances:\")\n",
        "for name, importance in zip(housing.feature_names, model.feature_importances_):\n",
        "    print(name, \":\", importance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gclZ6vhZDvbw",
        "outputId": "239bdf9f-a253-45e6-dee9-8f9c4c17e81d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Parameter grid\n",
        "param_grid = {\n",
        "    \"max_depth\": [2, 3, 4, 5, None],\n",
        "    \"min_samples_split\": [2, 3, 4, 5]\n",
        "}\n",
        "\n",
        "# Grid Search\n",
        "grid = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters & accuracy\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SJp4o-jFAAH"
      },
      "source": [
        "**Question 10: Imagine you’re working as a data scientist for a healthcare company that**\n",
        "\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "* Handle the missing values\n",
        "* Encode the categorical features\n",
        "* Train a Decision Tree model\n",
        "* Tune its hyperparameters\n",
        "* Evaluate its performance\n",
        "\n",
        "\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRzgnVFLDLTS"
      },
      "source": [
        "**Answer**\n",
        "\n",
        "**Healthcare Disease Prediction Project**\n",
        "\n",
        "**1. Handle Missing Values**\n",
        "\n",
        "* Numerical features → use mean/median imputation\n",
        "\n",
        "* Categorical features → use most frequent value\n",
        "\n",
        "* Use sklearn’s SimpleImputer\n",
        "\n",
        "**2. Encode Categorical Features For Decision Trees:**\n",
        "\n",
        "* One-Hot Encoding (preferred) Or Ordinal Encoding\n",
        "\n",
        "* Use OneHotEncoder inside a ColumnTransformer.\n",
        "\n",
        "**3. Train a Decision Tree Model Steps:**\n",
        "\n",
        "* Split data into train/test\n",
        "\n",
        "* Apply preprocessing\n",
        "\n",
        "* Train DecisionTreeClassifier()\n",
        "\n",
        "* Evaluate using accuracy, precision, recall, F1-score\n",
        "\n",
        "**4. Hyperparameter Tuning Use GridSearchCV with parameters:**\n",
        "\n",
        "* max_depth\n",
        "\n",
        "* min_samples_split\n",
        "\n",
        "* min_samples_leaf\n",
        "\n",
        "* criterion (gini/entropy)\n",
        "\n",
        "**5. Evaluate Performance Use:**\n",
        "\n",
        "* Confusion Matrix\n",
        "\n",
        "* ROC-AUC\n",
        "\n",
        "* Classification Report\n",
        "\n",
        "**6. Business Value The model helps:**\n",
        "\n",
        "* Early disease detection\n",
        "\n",
        "* Reduce manual diagnosis workload\n",
        "\n",
        "* Prioritize high-risk patients\n",
        "\n",
        "* Save medical resources\n",
        "\n",
        "* Improve patient outcomes\n",
        "\n",
        "* Make data-driven healthcare decisions"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}